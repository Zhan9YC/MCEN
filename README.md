# Multilevel information compression and textual information enhancement for multimodal sentiment analysis
Pytorch implementation of paper:
## Multilevel information compression and textual information enhancement for multimodal sentiment analysis
https://doi.org/10.1016/j.knosys.2025.113121

# code
coming soon

# Citation
* Multilevel information compression and textual information enhancement for multimodal sentiment analysis
  Please cite our paper if you find our work useful for your research:
  @article{ZHANG2025113121,
title = {Multilevel information compression and textual information enhancement for multimodal sentiment analysis},
journal = {Knowledge-Based Systems},
volume = {312},
pages = {113121},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113121},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125001686},
author = {Yuchen Zhang and Hong Zhong and Naji Alhusaini and Guilin Chen and Cheng Wu},
keywords = {Multi-layer feature utilization, Information fusion, Multimodal sentiment analysis, Noise reduce},
abstract = {Multimodal sentiment analysis (MSA) aims to extract sentiment information from textual, visual, and auditory modalities. While previous works have largely focused on representation learning and feature fusion strategies, challenges unique to video-based multimodal data remain underexplored. Compared to image–text tasks, video sequences are longer, leading to increased redundancy and noise in both visual and auditory modalities. Additionally, there are inherent discrepancies in semantic richness across different modalities, a factor often overlooked in prior studies. To address these challenges, we propose the Multi-level Information Compression and Textual Information Enhancement Network (MCEN), which leverages the inherent advantages of textual data in MSA tasks. Our approach features a Hierarchical Information Compression (HIC) module designed to reduce noise and redundancy in the visual and auditory modalities while extracting unimodal semantic information at varying scales. Furthermore, we introduce a Textual Information Enhancement (TIE) module to facilitate cross-modal information fusion at different semantic levels. The model is further enhanced by a Contrastive Predictive Coding (CPC) loss, which regulates the contribution of different layers and improves the inter-modal information used. Extensive experiments conducted on three widely used benchmark datasets — MOSI, MOSEI, and CH-SIMS — demonstrate that our method achieves state-of-the-art performance while maintaining a parameter count under 1M.}
}
